# crt_c_reviewer

an AI reviewer for aws-c code.

The biggest news for aws-common-runtime for this year is @graebm left the teamðŸ¥², who is our beloved engineer and best in the world code reviewer.
Github Copliot provides code review functionalities, but it cannot be used.

So, this project is trying to learn from the best code reviewer and try to do some code review jobs using AI agent.

## What is MCP

https://github.com/modelcontextprotocol
https://modelcontextprotocol.io/introduction

My summarize. The protocol an AI agent can talk with each other. Services can provide their MCP server that let agent know what's the best way to talk with the service. And client can loads MCP server, and knows how to use those services.

## MCP client used

There is bunch of clients https://modelcontextprotocol.io/clients, includes AmazonQ, and some VScode plugins. I tried couple, and they are somewhat limited and mostly not be able to code.

In the end, I found https://github.com/evalstate/fast-agent, it supports all features and it's open sourced.

So, I fork on fast agent, and uses bedrock claud and fetch your AWS credentials from env, using crt, instead of claud directly. So that we are fully AWS based.
https://github.com/TingDaoK/fast-agent

## MCP server used

There are couple example server listed by the doc https://modelcontextprotocol.io/examples, and there are more from the communities. In this project, I used:

1. It uses MCP server `@modelcontextprotocol/server-filesystem` to access file system, https://github.com/modelcontextprotocol/servers/tree/main/src/filesystem
2. Uses `@modelcontextprotocol/server-github` to performance github related access and action, https://github.com/modelcontextprotocol/servers/tree/main/src/github
3. Use `awslabs.bedrock-kb-retrieval-mcp-server@latest` to retrieve from bedrock knowledge base, https://awslabs.github.io/mcp/servers/bedrock-kb-retrieval-mcp-server/
4. `aws_mcp_server` to use aws-cli, https://github.com/alexei-led/aws-mcp-server

## How to run it

* you need aws credentials environment that has access to `anthropic.claude-3-7-sonnet-20250219-v1:0`, which is the model I hard coded in the forked repo. You could modify the model from this line, [here](https://github.com/TingDaoK/fast-agent/commit/8739a627ab3f8cfd145a17b54196ee23dbe60dc4#diff-fb1ce54f38f8793936d51c528c696b2339217a2351265b7b3042a831b141fa24R160)
* follow the instruction for `fast-agent` [here](https://github.com/TingDaoK/fast-agent?tab=readme-ov-file#get-started), but install it directly from the forked source, instead of pypi.

```
git clone https://github.com/TingDaoK/fast-agent.git && cd fast-agent
uv pip install awscrt
uv pip install -e .
```

* Update the `GITHUB_PERSONAL_ACCESS_TOKEN` in fastagent.config.yaml, if you need the agent to post PR reviews for you, you will need to grant it the permission from the token.

* choose the agent to run. There are bunch of hard-coded path to my local env, so, update the path.

## Step by Step

### Gather data

* `pr_fetcher.py` , this agent firstly pulls the last 20 Pull request @graebm made comments on and save it to a file
* `comment_fetcher.py`, this agent Given a of a github pull request, fetch the review comments on the pull requests and get the comments @graebm made and save them into a local file as agent_comments with the Pull request number under raw_comments/ directory.

So, with those two agents, we get all the comments from @graebm on the last 20 PR he commented at on aws-c-s3.

### "Train" the agent

* `train_based_on_comments.py`, this agent takes each comments generated by the previous step and `Summarize it as detail as you can so that next time you will make comments like it` for each of the comments, it uses `parallel` to make things a bit faster, but the downside is hard coded path for each file. Maybe there is a better way, but I haven't dig deep
* `summarize_all_comments_analysis.py`, the agent takes the analysis for all 20 PR comments, and summarize all of them. Now we have `comments_styles.md` as the guidance to review code like @graebm.

### Review the code

* Before we review the code, I learnt there is a bedrock knowledge base, which should be a helper resource for AI to know our code base. So, I took all the C code we have and uploaded it to S3 and create a knowledge base with it.
* Now, the agent is supposedly has couple abilities:
1. Fetch the code change from the pull request.
2. Use the knowledge base to know the code base for any relative code.
3. Knows how @graebm comments on code by the summarization made before.
4. It can actually post the review by itself given enough permission.
Which should be a bit helpful for it as the code reviewer to look into aws-c code base!.

However, it's not that easy.
I firstly tried:
* `reviewer.py`, I started to try to let the agent review the code based on the resource I give. There are couple unexpected things:
   1. The position information is obviously wrong.
   2. It failed to detect a very simple memory leak.
   3. a lot of positive and not very useful comments. People like some positive comments, but too many is just annoying,
Time to prompt engineering.
* Split up the reviewer to focus on different aspect of the code
    1. naming!
    2. Documentation!
    3. Implementation details!
        - API usage!
        - Error handling!
        - Memory safety!
        - code simplicity!

The prompt engineering is not easy to get my expected result. The lines are still very wrong, I guess somewhere the line information was lost, and GenAI just make things up when I ask for the exactly line.

But, I found the naming and code simplicity are helpful.

* Lastly, if you really want the agent the do the work for you, `review_summarize.py` is able to look into the review files and put them together and submit them on your behave!

## Summarize

### Positive

* It did decent job to grab from github and all the comments as I asked for
* I also like how it summarize the comments and some of those even dig deep into why those comments are made.
* I do like the agent's suggestion on naming, catch typos and the code simplicity suggestions. Those suggestions are actually productive! I am no good at naming at all, but AI does give some productive suggestions about naming and documentations. It also does a decent job about code simplicity.

### Negative

* This AI code reviewer is not near our best code reviewer. I tried it on some of our real Pull Request, when the code is complicated, it's hard to catch any bugs.
* Most of the comments are not very helpful, it takes time to check each of them
* The line comment about is mostly wrong. So, make the comments hard to follow. I tried, and failed.
* It takes a long time to process, couple minutes to review the code.
* The result is not very consistent. Sometimes, it can catch the bug but sometimes not with the same code and same prompt.

### In the end

The MCP is very powerful! Service can add their knowledge to let AI handles the job more efficiently. Will it even be a replacement of SDK? Maybe, if everyone starts to use AI to interact with services, maybe most of the SDK models are not needed anymore. But, it's still in a very early stage.

How much it can help to review the code? I think it's still very minimal, but a good start. I do find things that are productive. And with more prompt engineering, I think it can get better.
The model doesn't enable thinking for now, with thinking, it maybe more productive?
