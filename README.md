# crt_c_reviewer

an AI reviewer for aws-c code.

The biggest news for aws-common-runtime for this year is @graebm left the teamðŸ¥², who is our beloved engineer and best in the world code reviewer.
Github Copliot provides code review functionalities, but it cannot be used.

So, this project is trying to learn from the best code reviewer and try to do some code review jobs using AI agent.

## MCP client used

My fork on fast agent, which uses bedrock claud and fetch your AWS credentials from env, using crt, instead of claud directly. So that we are fully AWS based.
https://github.com/TingDaoK/fast-agent

## How to run it

* you need aws credentials environment that has access to `anthropic.claude-3-7-sonnet-20250219-v1:0`, which is the model I hard coded in the forked repo. You could modify the model from this line, [here](https://github.com/TingDaoK/fast-agent/commit/8739a627ab3f8cfd145a17b54196ee23dbe60dc4#diff-fb1ce54f38f8793936d51c528c696b2339217a2351265b7b3042a831b141fa24R160)
* follow the instruction for `fast-agent` [here](https://github.com/TingDaoK/fast-agent?tab=readme-ov-file#get-started), but install it directly from the forked source, instead of pypi.

```
git clone https://github.com/TingDaoK/fast-agent.git && cd fast-agent
uv pip install awscrt
uv pip install -e .
```

* Update the `GITHUB_PERSONAL_ACCESS_TOKEN` in fastagent.config.yaml, if you need the agent to post PR reviews for you, you will need to grant it the permission from the token.

* choose the agent to run. There are bunch of hard-coded path to my local env, so, update the path.

## MCP server used

1. It uses MCP server `@modelcontextprotocol/server-filesystem` to access file system
2. Uses `@modelcontextprotocol/server-github` to performance github related access and action
3. Use `awslabs.bedrock-kb-retrieval-mcp-server@latest` to retrieve from bedrock knowledge base.
4. `aws_mcp_server` to use aws-cli

## Step by Step

### Gather data

* `pr_fetcher.py` , this agent firstly pulls the last 20 Pull request @graebm made comments on and save it to a file
* `comment_fetcher.py`, this agent Given a of a github pull request, fetch the review comments on the pull requests and get the comments @graebm made and save them into a local file as agent_comments with the Pull request number under raw_comments/ directory.

So, with those two agents, we get all the comments from @graebm on the last 20 PR he commented at on aws-c-s3.

### "Train" the agent

* `train_based_on_comments.py`, this agent takes each comments generated by the previous step and `Summarize it as detail as you can so that next time you will make comments like it` for each of the comments, it uses `parallel` to make things a bit faster, but the downside is hard coded path for each file. Maybe there is a better way, but I haven't dig deep
* `summarize_all_comments_analysis.py`, the agent takes the analysis for all 20 PR comments, and summarize all of them. Now we have `comments_styles.md` as the guidance to review code like @graebm.

### Review the code

* Before we review the code, I learnt there is a bedrock knowledge base, which should be a helper resource for AI to know our code base. So, I took all the C code we have and uploaded it to S3 and create a knowledge base with it.
* Now, the agent is supposedly has couple abilities:
1. Fetch the code change from the pull request.
2. Use the knowledge base to know the code base for any relative code.
3. Knows how @graebm comments on code by the summarization made before.
4. It can actually post the review by itself given enough permission.
Which should be a bit helpful for it as the code reviewer to look into aws-c code base!.

However, it's not that easy.
I firstly tried:
* `reviewer.py`, I started to try to let the agent review the code based on the resource I give. There are couple unexpected things:
   1. The position information is obviously wrong.
   2. It failed to detect a very simple memory leak.
   3. a lot of positive and not very useful comments. People like some positive comments, but too many is just annoying,
Time to prompt engineering.
* Split up the reviewer to focus on different aspect of the code
    1. naming!
    2. Documentation!
    3. Implementation details!
        - API usage!
        - Error handling!
        - Memory safety!
        - code simplicity!

The prompt engineering is not easy to get my expected result. The lines are still very wrong, I guess somewhere the line information was lost, and GenAI just make things up when I ask for the exactly line.

But, I found the naming and code simplicity are helpful.

* Lastly, if you really want the agent the do the work for you, `review_summarize.py` is able to look into the review files and put them together and submit them on your behave!
